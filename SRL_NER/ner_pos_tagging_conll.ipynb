{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 5\n",
    "\n",
    "**Tasks :- NER tagging, POS tagging**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``NER`` :-This is a Named Entity Recognition task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\".\n",
    "\n",
    "``POS`` :- This is a Part of Speech tagging task. A part of speech is a category of words that have similar grammatical properties. Each word of the sentence is tagged with the part of speech label it belongs to. The words which don't belong to any part of speech label are simply labeled as \"O\".\n",
    "\n",
    "**Conversational Utility** :-  In conversational AI context, determining the syntactic parts of the sentence can help in extracting noun-phrases or important keyphrases from the sentence.\n",
    "\n",
    "**Data** :- In this example, we are using the <a href=\"https://www.clips.uantwerpen.be/conll2003/ner/\">coNLL 2003</a> data which is BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "The data is already present in ``coNLL_data`` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "Raw data is in BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "We already provide a sample transformation function ``coNLL_ner_pos_to_tsv`` to convert this data to required tsv format. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_conll.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: coNLL_ner_pos_to_tsv\n",
    "  read_file_names:\n",
    "    - coNLL_train.txt\n",
    "    - coNLL_testa.txt\n",
    "    - coNLL_testb.txt\n",
    "  read_dir: coNLL_data\n",
    "  save_dir: ../../data\n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#Vô powershell (PS), nếu đang ở thư mục gốc thì copy vô PS chạy lệnh ở dưới này lên trước, còn nếu ở trong thư mục SRL_DEPENDENCY rồi thì thôi\n",
    "cd .\\SRL_NER\n",
    "#Lệnh chính\n",
    "python ..\\data_transformations.py --transform_file transform_file_conll.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "For more details on the data preparation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for entailment task. The file is already created at ``tasks_file_conll.yml``\n",
    "```\n",
    "conllner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.2\n",
    "  label_map_or_file: ../../data/ner_coNLL_train_label_map.joblib\n",
    "  metrics:\n",
    "  - seqeval_f1_score\n",
    "  - seqeval_precision\n",
    "  - seqeval_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_coNLL_train.tsv\n",
    "  - ner_coNLL_testa.tsv\n",
    "  - ner_coNLL_testb.tsv\n",
    "\n",
    "conllpos:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    label_map_or_file: ../../data/pos_coNLL_train_label_map.joblib\n",
    "    metrics:\n",
    "    - seqeval_f1_score\n",
    "    - seqeval_precision\n",
    "    - seqeval_recall\n",
    "    loss_type: NERLoss\n",
    "    task_type: NER\n",
    "    file_names:\n",
    "    - pos_coNLL_train.tsv\n",
    "    - pos_coNLL_testa.tsv\n",
    "    - pos_coNLL_testb.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#Nếu đang ở thư mục SRL_DEPENDENCY_MODEL thì khỏi chạy lệnh này\n",
    "cd .\\SRL_NER\n",
    "#Lệnh chính\n",
    "python ..\\data_preparation.py --task_file tasks_file_conll.yml --data_dir .\\data_to_prepare --max_seq_len 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -3 Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#Nếu đang ở thư mục SRL_DEPENDENCY_MODEL thì khỏi chạy lệnh này\n",
    "cd .\\SRL_NER\n",
    "#Lệnh chính\n",
    "python ..\\train.py --data_dir .\\data_to_prepare\\dmis-lab\\biobert-base-cased-v1.2_prepared_data --task_file tasks_file_conll.yml --out_dir conll_ner_pos_biobert_base --epochs 1 --train_batch_size 32 --eval_batch_size 32 --grad_accumulation_steps 1 --log_per_updates 50 --max_seq_len 50 --eval_while_train --test_while_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n",
    "The trained model and maximum sequence length to be used needs to be specified.\n",
    "\n",
    "For knowing more details about infering, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/infering.html\">infer pipeline</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from infer_pipeline import inferPipeline\n",
    "\n",
    "pipe = inferPipeline(modelPath=\"./conll_ner_pos_biobert_base/multi_task_model_0_1123.pt\", maxSeqLen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 2it [00:00, 10.51it/s]                       \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "length of sample and result list not same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m samples \u001b[39m=\u001b[39m [ [sample_sentence_1], [sample_sentence_2] ]\n\u001b[0;32m      6\u001b[0m tasks \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mconllSRL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconllNER\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m pipe\u001b[39m.\u001b[39;49minfer(samples, tasks)\n",
      "File \u001b[1;32me:\\Code\\GitHub\\multitask_SRL_NER\\SRL_NER\\..\\infer_pipeline.py:294\u001b[0m, in \u001b[0;36minferPipeline.infer\u001b[1;34m(self, dataList, taskNamesList, batchSize, seed)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m    290\u001b[0m     allIds, allPreds, allScores \u001b[39m=\u001b[39m evaluate(allData, batchSampler, inferDataLoader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtaskParams,\n\u001b[0;32m    291\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, gpu\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available(), evalBatchSize\u001b[39m=\u001b[39mbatchSize, needMetrics\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, hasTrueLabels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m             returnPred\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m     finalOutList \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_output(dataList, allIds, allPreds, allScores)\n\u001b[0;32m    295\u001b[0m     \u001b[39m#print(finalOutList)\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m finalOutList\n",
      "File \u001b[1;32me:\\Code\\GitHub\\multitask_SRL_NER\\SRL_NER\\..\\infer_pipeline.py:181\u001b[0m, in \u001b[0;36minferPipeline.format_output\u001b[1;34m(self, dataList, allIds, allPreds, allScores)\u001b[0m\n\u001b[0;32m    179\u001b[0m     inpp \u001b[39m=\u001b[39m dataList[sampleId][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit()\n\u001b[0;32m    180\u001b[0m     \u001b[39m#print(\"{} : \".format(taskName))\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_ner_output(inpp, result)\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     result \u001b[39m=\u001b[39m [allPreds[i][sampleId], allScores[i][sampleId]]\n",
      "File \u001b[1;32me:\\Code\\GitHub\\multitask_SRL_NER\\SRL_NER\\..\\infer_pipeline.py:142\u001b[0m, in \u001b[0;36minferPipeline.format_ner_output\u001b[1;34m(self, sample, result)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_ner_output\u001b[39m(\u001b[39mself\u001b[39m, sample, result):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(sample) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(result), \u001b[39m\"\u001b[39m\u001b[39mlength of sample and result list not same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m     returnList \u001b[39m=\u001b[39m []\n\u001b[0;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m i, (sam, res) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(sample, result)):\n",
      "\u001b[1;31mAssertionError\u001b[0m: length of sample and result list not same"
     ]
    }
   ],
   "source": [
    "sample_sentence_1 = \"normal lhx4 splicing was abolished by this intronic mutation , which segregates in a dominant and fully penetrant manner over three generations and activates two exonic cryptic splice sites , thereby predicting two different proteins deleted in their homeodomain sequence.\"\n",
    "\n",
    "sample_sentence_2 = \"to determine how activators interact with the complex and to examine the importance of these interactions , relative to other potential targeting mechanisms , for swi/snf function , we found to identify and mutate amino acids ee(437 and 438).\"\n",
    "\n",
    "samples = [ [sample_sentence_1], [sample_sentence_2] ]\n",
    "tasks = ['conllSRL', 'conllNER']\n",
    "pipe.infer(samples, tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
