{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 1\n",
    "\n",
    "**Tasks :- Intent Detection, NER, Fragment Detection**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``Intent Detection`` :- This is a single sentence classification task where an `intent` specifies which class the data sample belongs to. \n",
    "\n",
    "``NER`` :- This is a Named Entity Recognition/ Sequence Labelling/ Slot filling task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\". \n",
    "\n",
    "``Fragment Detection`` :- This is modeled as a single sentence classification task which detects whether a sentence is incomplete (fragment) or not (non-fragment).\n",
    "\n",
    "**Conversational Utility** :-  Intent detection is one of the fundamental components for conversational system as it gives a broad understand of the category/domain the sentence/query belongs to.\n",
    "\n",
    "NER helps in extracting values for required entities (eg. location, date-time) from query.\n",
    "\n",
    "Fragment detection is a very useful piece in conversational system as knowing if a query/sentence is incomplete can aid in discarding bad queries beforehand.\n",
    "\n",
    "\n",
    "**Data** :- In this example, we are using the <a href=\"https://snips-nlu.readthedocs.io/en/latest/dataset.html\">SNIPS</a> data for intent and entity detection. For the sake of simplicity, we provide \n",
    "the data in simpler form under ``snips_data`` directory taken from <a href=\"https://github.com/LeePleased/StackPropagation-SLU/tree/master/data/snips\">here</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "The data is present in *BIO* format where each word in a sentence is tagged with corresponding entity. \n",
    "Sentences are separated by \\\" \" and at the end of each sentence, intent class to which the sentence belongs is mentioned. We already provide a sample transformation function ``snli_entailment_to_tsv`` to convert this data to required tsv data files. T\n",
    "Fragment detection data is generated from intent detection data created using the transform function\n",
    "``create_fragment_detection_tsv``. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_snips.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: snips_intent_ner_to_tsv\n",
    "  read_file_names:\n",
    "    - snips_train.txt\n",
    "    - snips_dev.txt\n",
    "    - snips_test.txt\n",
    "  read_dir: snips_data\n",
    "  save_dir: ../../data\n",
    "  \n",
    "transform2:\n",
    "  transform_func: create_fragment_detection_tsv\n",
    "  read_file_names:\n",
    "    - intent_snips_train.tsv\n",
    "    - intent_snips_dev.tsv\n",
    "    - intent_snips_test.tsv\n",
    "  read_dir: ../../data\n",
    "  save_dir: ../../data\n",
    "  \n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../data_transformations.py \\\n",
    "    --transform_file 'transform_file_snips.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "Here we are training the three tasks together for demonstration. This means we will have a single\n",
    "multi-task model capable of performing on all the three tasks. You can also train the tasks separately \n",
    "by mentioning single tasks in task file.\n",
    "\n",
    "For more details on the data preparation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for multiple tasks - intent detection, NER and fragment detection. The file is already created at ``tasks_file_snips.yml``\n",
    "\n",
    "```\n",
    "ner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.3\n",
    "  label_map_or_file: ../../data/ner_snips_train_label_map.joblib\n",
    "  metrics:\n",
    "  - snips_f1_score\n",
    "  - snips_precision\n",
    "  - snips_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_snips_train.tsv\n",
    "  - ner_snips_dev.tsv\n",
    "  - ner_snips_test.tsv\n",
    "\n",
    "intent:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.3\n",
    "    label_map_or_file: ../../data/int_snips_train_label_map.joblib\n",
    "    metrics:\n",
    "    - classification_accuracy\n",
    "    loss_type: CrossEntropyLoss\n",
    "    task_type: SingleSenClassification\n",
    "    file_names:\n",
    "    - int_snips_train.tsv\n",
    "    - int_snips_dev.tsv\n",
    "    - int_snips_test.tsv\n",
    "\n",
    "    \n",
    "fragment:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    class_num: 2\n",
    "    metrics:\n",
    "    - classification_accuracy\n",
    "    loss_type: CrossEntropyLoss\n",
    "    task_type: SingleSenClassification\n",
    "    file_names:\n",
    "    - fragment_snips_train.tsv\n",
    "    - fragment_snips_dev.tsv\n",
    "    - fragment_snips_test.tsv\n",
    "```\n",
    "\n",
    "Following command can be used to run the data preparation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../data_preparation.py \\\n",
    "    --task_file 'tasks_file_snips.yml' \\\n",
    "    --data_dir '../../data' \\\n",
    "    --max_seq_len 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 3 Running train\n",
    "\n",
    "Following command will start the training for the tasks. The log file reporting the loss, metrics and the tensorboard logs will be present in a time-stamped directory. For demonstration, we've put up sample logs under ``train_logs`` directory.\n",
    "\n",
    "For knowing more details about the train process, refer to <a href= \"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-train\">running training</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../train.py \\\n",
    "    --data_dir '../../data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_snips.yml' \\\n",
    "    --out_dir 'snips_intent_ner_fragment_bert_base' \\\n",
    "    --epochs 3 \\\n",
    "    --train_batch_size 16 \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --grad_accumulation_steps 2 \\\n",
    "    --log_per_updates 50 \\\n",
    "    --max_seq_len 50 \\\n",
    "    --eval_while_train \\\n",
    "    --test_while_train \\\n",
    "    --silent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n",
    "The trained model and maximum sequence length to be used needs to be specified.\n",
    "\n",
    "For knowing more details about infering, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/infering.html\">infer pipeline</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
