{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 5\n",
    "\n",
    "**Tasks :- NER tagging, POS tagging**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``NER`` :-This is a Named Entity Recognition task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\".\n",
    "\n",
    "``POS`` :- This is a Part of Speech tagging task. A part of speech is a category of words that have similar grammatical properties. Each word of the sentence is tagged with the part of speech label it belongs to. The words which don't belong to any part of speech label are simply labeled as \"O\".\n",
    "\n",
    "**Conversational Utility** :-  In conversational AI context, determining the syntactic parts of the sentence can help in extracting noun-phrases or important keyphrases from the sentence.\n",
    "\n",
    "**Data** :- In this example, we are using the <a href=\"https://www.clips.uantwerpen.be/conll2003/ner/\">coNLL 2003</a> data which is BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "The data is already present in ``coNLL_data`` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "Raw data is in BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "We already provide a sample transformation function ``coNLL_ner_pos_to_tsv`` to convert this data to required tsv format. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_conll.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: coNLL_ner_pos_to_tsv\n",
    "  read_file_names:\n",
    "    - coNLL_train.txt\n",
    "    - coNLL_testa.txt\n",
    "    - coNLL_testb.txt\n",
    "  read_dir: coNLL_data\n",
    "  save_dir: ../../data\n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../data_transformations.py \\\n",
    "    --transform_file 'transform_file_conll.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "For more details on the data preparation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for entailment task. The file is already created at ``tasks_file_conll.yml``\n",
    "```\n",
    "conllner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.2\n",
    "  label_map_or_file: ../../data/ner_coNLL_train_label_map.joblib\n",
    "  metrics:\n",
    "  - seqeval_f1_score\n",
    "  - seqeval_precision\n",
    "  - seqeval_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_coNLL_train.tsv\n",
    "  - ner_coNLL_testa.tsv\n",
    "  - ner_coNLL_testb.tsv\n",
    "\n",
    "conllpos:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    label_map_or_file: ../../data/pos_coNLL_train_label_map.joblib\n",
    "    metrics:\n",
    "    - seqeval_f1_score\n",
    "    - seqeval_precision\n",
    "    - seqeval_recall\n",
    "    loss_type: NERLoss\n",
    "    task_type: NER\n",
    "    file_names:\n",
    "    - pos_coNLL_train.tsv\n",
    "    - pos_coNLL_testa.tsv\n",
    "    - pos_coNLL_testb.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../data_preparation.py \\\n",
    "    --task_file 'tasks_file_conll.yml' \\\n",
    "    --data_dir '../../data' \\\n",
    "    --max_seq_len 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -3 Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../train.py \\\n",
    "    --data_dir '../../data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_conll.yml' \\\n",
    "    --out_dir 'conll_ner_pos_bert_base' \\\n",
    "    --epochs 10 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --grad_accumulation_steps 1 \\\n",
    "    --log_per_updates 50 \\\n",
    "    --max_seq_len 50 \\\n",
    "    --eval_while_train \\\n",
    "    --test_while_train \\\n",
    "    --silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n",
    "The trained model and maximum sequence length to be used needs to be specified.\n",
    "\n",
    "For knowing more details about infering, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/infering.html\">infer pipeline</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
